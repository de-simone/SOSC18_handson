{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "***\n",
    "# <font color=blue>MACHINE LEARNING ESSENTIALS</font>\n",
    "# <font color=blue>Practice with Logistic Regression</font>\n",
    "# <font color=blue>(student version)</font>\n",
    "<div style=\"text-align: right\"><font color=magenta>Andrea De Simone</font></div>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import optimize as opt\n",
    "from scipy.special import binom as binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "#import os\n",
    "data1 = np.loadtxt('dataset1.csv', delimiter=',')\n",
    "print(data1[:10])\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate features (x) from labels (y)\n",
    "x=data1[:,0:2]\n",
    "y=data1[:,2]\n",
    "\n",
    "# Number of examples, Dimensions of feature space\n",
    "N, D = x.shape\n",
    "\n",
    "y = np.reshape(y,(N,1))   # re-shape y to column vector\n",
    "print(y[:10])\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"N. of examples in class 0 =\",np.sum(y==0))\n",
    "print(\"N. of examples in class 1 =\",np.sum(y==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add column of ones in front of x\n",
    "ones = np.ones((N,1))\n",
    "X = np.concatenate((ones,x),axis=1)\n",
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Data \n",
    "fig, ax = plt.subplots()  \n",
    "positives = np.ravel(y==1)\n",
    "negatives = np.ravel(y==0)\n",
    "ax.scatter(X[positives,1], X[positives,2], s=20, c='magenta', marker='o', label='Admitted')  \n",
    "ax.scatter(X[negatives,1], X[negatives,2], s=20, c='blue', marker='x', label='Not Admitted')  \n",
    "leg = ax.legend(frameon=True, loc='upper right')  \n",
    "ax.set_xlabel('Exam 1 Score')  \n",
    "ax.set_ylabel('Exam 2 Score')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return( 1.0 / (1 + np.exp(-z)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot sigmoid function\n",
    "xaxis = np.arange(-10, 10, step=.1)\n",
    "fig, ax = plt.subplots()  \n",
    "ax.plot(xaxis, sigmoid(xaxis), 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_theta=np.zeros((D+1,1))\n",
    "print(initial_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X.shape,y.shape, initial_theta.shape)\n",
    "print(np.dot(X,initial_theta).shape)\n",
    "print(y.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def Loss(theta, X, y):  \n",
    "    \n",
    "    n = y.shape[0]\n",
    "    h = sigmoid(np.dot(X,theta))\n",
    "\n",
    "    loss = (1/float(n)) *  (-np.dot(y.T,np.log(h)) - np.dot((1 - y).T,(np.log(1 - h))))\n",
    "    \n",
    "    loss = np.ravel(loss)[0]\n",
    "    return( loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Loss(initial_theta,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient of the Loss function\n",
    "def LossGradient(theta, X, y):\n",
    "    \n",
    "    n, d = X.shape\n",
    "    y = y.reshape((n, 1)) \n",
    "    theta = theta.reshape((d, 1)) \n",
    "    \n",
    "    h = sigmoid(np.dot(X,theta)) \n",
    " \n",
    "    grad = (1/float(n)) * np.dot(X.T, h-y)\n",
    "    \n",
    "    grad = np.ravel(grad)\n",
    "    return( grad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LossGradient(initial_theta,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GradientDescent(X,y, num_steps, alpha):\n",
    "    \n",
    "    n, d = X.shape\n",
    "    \n",
    "    weights = np.zeros((d,1))\n",
    "    epsilon=0.0\n",
    "    #weights = np.random.rand(d,1)*epsilon\n",
    "\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        grad = LossGradient(weights,X,y).reshape((d,1))\n",
    "        weights -= alpha * grad\n",
    "        # Print Loss\n",
    "        #if step % 1000 == 0:\n",
    "        #    print(Loss(weights,X, y))\n",
    "    \n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_hat_GD = GradientDescent(X,y,10000,5e-4)\n",
    "print(theta_hat_GD)\n",
    "print(Loss(theta_hat_GD,X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>>>> Q1: Run Gradient Descent varying the number of iterations and the learning rate: What is the minimum value of the Loss function you can get?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize loss function with scipy minimizer\n",
    "def MinimizeLoss(theta, X, y):\n",
    "     \n",
    "    result = opt.minimize(fun = Loss, x0 = theta, args = (X, y),\n",
    "                         method = 'TNC', jac = LossGradient)\n",
    "    print(result)\n",
    "    return(result.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "initial_theta = np.zeros((X.shape[1],1))\n",
    "theta_hat = MinimizeLoss(initial_theta,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"theta = \", theta_hat)\n",
    "print(\"Loss = \", Loss(theta_hat, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Predicted Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "def predict(theta, X, threshold=0.5):  \n",
    "    probability = sigmoid(np.dot(X,theta.T))\n",
    "    prediction = [1 if x >= threshold else 0 for x in probability]\n",
    "    return(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = 0.5\n",
    "y_predicted = predict(theta_hat, X, threshold=T)  \n",
    "y_predicted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TP = np.sum( [ (pred==1 and true==1) for (pred,true) in zip(y_predicted,  np.ravel(y))] )\n",
    "TN = np.sum( [ (pred==0 and true==0) for (pred,true) in zip(y_predicted,  np.ravel(y))] )\n",
    "FP = np.sum( [ (pred==1 and true==0) for (pred,true) in zip(y_predicted,  np.ravel(y))] )\n",
    "FN = np.sum( [ (pred==0 and true==1) for (pred,true) in zip(y_predicted,  np.ravel(y))] )\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print([TP, FP]) \n",
    "print([FN, TN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>>>> Q2: Compute: Accuracy, Precision, Recall, F-score</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start Edit...\n",
    "accuracy = 0.0 #?\n",
    "precision = 0.0 #?\n",
    "recall = 0.0 #?\n",
    "Fscore = 0.0 #?\n",
    "# ... End Edit\n",
    "\n",
    "print('accuracy  = {:.2f}'.format(accuracy))\n",
    "print('precision = {:.2f}'.format(precision))\n",
    "print('recall    = {:.2f}'.format(recall))\n",
    "print('F-score   = {:.2f}'.format(Fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>>>> Q3: Change discrimination threshold to 0.3. What happens to Precision, Recall, F-score?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>>>> Q4: Change discrimination threshold to 0.9. What happens to Precision, Recall, F-score?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>>>> Q5: Record True Positive Rate and False Positive Rate for 5 different values of the threshold and plot the ROC curve</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TPR_list = []\n",
    "FPR_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PlotROC(FPR_list, TPR_list):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(\"ROC curve\")\n",
    "    ax.plot(FPR_list,TPR_list, marker=\"x\", c=\"b\")\n",
    "    ax.plot(FPR_list,FPR_list, marker=\"\", c=\"black\",linestyle='dashed',alpha=0.5)\n",
    "    ax.tick_params(labelsize=12)\n",
    "    ax.set_xlabel(\"FPR\", fontsize=14)\n",
    "    ax.set_ylabel(\"TPR\",fontsize=14)\n",
    "    ax.set_xlim([-0.05,1.05])\n",
    "    ax.set_ylim([-0.05,1.05])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PlotROC(FPR_list, TPR_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Plot Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PlotDecisionBoundary(XX,Y,theta, degree=2):\n",
    "    fig, ax = plt.subplots()  \n",
    "\n",
    "    # plot examples\n",
    "    positives = np.ravel(Y==1)\n",
    "    negatives = np.ravel(Y==0)\n",
    "    ax.scatter(XX[positives,1], XX[positives,2], s=20, c='magenta', marker='o', label='y=1')  \n",
    "    ax.scatter(XX[negatives,1], XX[negatives,2], s=20, c='blue', marker='x', label='y=0')  \n",
    "    leg = ax.legend(frameon=True, loc='upper right')  \n",
    "    ax.set_xlabel('x1')  \n",
    "    ax.set_ylabel('x2')  \n",
    "    \n",
    "    # plot decision boundary (threshold=0.5)\n",
    "    h = 0.01  # step size in the mesh\n",
    "    x_min, x_max = XX[:, 1].min() - 0.5, XX[:, 1].max() + 0.5\n",
    "    y_min, y_max = XX[:, 2].min() - 0.5, XX[:, 2].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    #ax.set_xlim(xx.min(), xx.max())\n",
    "    #ax.set_ylim(yy.min(), yy.max())\n",
    "    Xpoints = np.c_[np.ones(xx.ravel().shape[0]),xx.ravel(), yy.ravel()] \n",
    "    \n",
    "    if theta_hat.shape[0] <= 3:\n",
    "        Z = sigmoid(Xpoints.dot(theta))\n",
    "    else:\n",
    "        if int(binomial(2+degree,degree)==theta.shape[0]):\n",
    "            Xpoints_ext = MapFeatures(Xpoints[:,1],Xpoints[:,2],degree)\n",
    "            Z = sigmoid(Xpoints_ext.dot(theta))\n",
    "        else:\n",
    "            print(\"Dimensions of theta and X do not match!\")\n",
    "            print(\"Decision boundary not plotted\")\n",
    "            return\n",
    "        \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='green')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PlotDecisionBoundary(X,y,theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>>>> Q6: Complete function for Regularized Loss. <br>Evaluate it on 'theta_test', with regularization parameter = 0.1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regularized Loss function\n",
    "def RegLoss(theta, X, y, reg_param):\n",
    "    n = y.shape[0]\n",
    "    h = sigmoid(np.dot(X,theta))\n",
    "    \n",
    "    #Start Edit...\n",
    "\n",
    "    loss = 0.0 #?\n",
    "    # ... End Edit\n",
    "    \n",
    "    \n",
    "    loss = np.ravel(loss)[0]\n",
    "    return( loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_test=np.array([-2,0.02,0.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>>>> Q7: Complete function for Regularized Loss Gradient. <br>Evaluate it on 'theta_test', with regularization parameter = 0.1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient of the regularized loss function\n",
    "def RegLossGradient(theta, X, y, reg_param):\n",
    "    n, d = X.shape\n",
    "    y = y.reshape((n, 1)) \n",
    "    theta = theta.reshape((d, 1))\n",
    "    h = sigmoid(np.dot(X,theta)) \n",
    "    \n",
    "    # Start Edit...\n",
    "    \n",
    "    \n",
    "    grad = 0.0 #?\n",
    "    # ... End Edit\n",
    "    return( grad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data2 = np.loadtxt('dataset2.csv', delimiter=',')\n",
    "print(data2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate features (x) from labels (y)\n",
    "x=data2[:,0:2]\n",
    "y=data2[:,2]\n",
    "\n",
    "# Number of examples, Dimensions of feature space\n",
    "N, D = x.shape\n",
    "\n",
    "y = np.reshape(y,(N,1))   # re-shape y to column vector\n",
    "print(y[:10])\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"N. of examples in class 0 =\",np.sum(y==0))\n",
    "print(\"N. of examples in class 1 =\",np.sum(y==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add column of ones in front of x\n",
    "ones = np.ones((N,1))\n",
    "X = np.concatenate((ones,x),axis=1)\n",
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Data \n",
    "fig, ax = plt.subplots()  \n",
    "positives = np.ravel(y==1)\n",
    "negatives = np.ravel(y==0)\n",
    "ax.scatter(X[positives,1], X[positives,2], s=20, c='magenta', marker='o', label='y=1')  \n",
    "ax.scatter(X[negatives,1], X[negatives,2], s=20, c='blue', marker='x', label='y=0')  \n",
    "leg = ax.legend(frameon=True, loc='upper right')  \n",
    "ax.set_xlabel('x1')  \n",
    "ax.set_ylabel('x2')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Minimize regularized loss function with scipy minimizer\n",
    "def MinimizeRegLoss(theta, X, y, reg_param):\n",
    "     \n",
    "    result = opt.minimize(fun = RegLoss, x0 = theta, args = (X, y, reg_param),\n",
    "                         method = 'TNC', jac = RegLossGradient)\n",
    "    return(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_theta = np.zeros((X.shape[1],1))\n",
    "theta_hat = MinimizeRegLoss(initial_theta,X,y, 0.1)\n",
    "print(\"theta = \", theta_hat)\n",
    "print(\"Loss = \", RegLoss(theta_hat, X, y, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Predicted Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted = predict(theta_hat, X, threshold=0.5)  \n",
    "y_predicted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TP = np.sum( [ (pred==1 and true==1) for (pred,true) in zip(y_predicted,  np.ravel(y))] )\n",
    "TN = np.sum( [ (pred==0 and true==0) for (pred,true) in zip(y_predicted,  np.ravel(y))] )\n",
    "FP = np.sum( [ (pred==1 and true==0) for (pred,true) in zip(y_predicted,  np.ravel(y))] )\n",
    "FN = np.sum( [ (pred==0 and true==1) for (pred,true) in zip(y_predicted,  np.ravel(y))] )\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print([TP, FP]) \n",
    "print([FN, TN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "\n",
    "print('accuracy  = {:.2f}'.format(accuracy))\n",
    "print('precision = {:.2f}'.format(precision))\n",
    "print('recall    = {:.2f}'.format(recall))\n",
    "print('F-score   = {:.2f}'.format(Fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Plot Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PlotDecisionBoundary(X,y,theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Add More Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X\\to X_{\\rm ext}=\\left(\n",
    "\\begin{matrix}\n",
    "1, x_1 , x_2, x_1^2, x_1 x_2, x_2^2, \\ldots, x_1^5 x_2, x_1 x^5, x_1^6, x_2^6\n",
    "\\end{matrix}\n",
    "\\right)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MapFeatures(X1, X2, max_deg):\n",
    "    \"\"\"\n",
    "    Map features X1,X2 into all monomials of degree <= max_deg\n",
    "    \"\"\"\n",
    "    ncols = binomial(2+max_deg,max_deg) \n",
    "    result = np.ones( (X1.shape[0],int(ncols)) )\n",
    "\n",
    "    count=1\n",
    "    for i in range(1,max_deg+1):\n",
    "        for j in range(0,i+1):            \n",
    "            \n",
    "            Xnew = np.power(X1, i-j) * np.power(X2, j)\n",
    "            result[:,count]=Xnew\n",
    "            count += 1\n",
    "            #result = np.concatenate((result,Xnew),axis=1)\n",
    "            \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "degree = 6\n",
    "X_ext = MapFeatures(X[:,1],X[:,2], degree)\n",
    "X_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regularization_parameter = 0.1\n",
    "\n",
    "X_ext = MapFeatures(X[:,1],X[:,2],degree)\n",
    "initial_theta = np.zeros((X_ext.shape[1],1))\n",
    "\n",
    "theta_hat = MinimizeRegLoss(initial_theta,X_ext,y, regularization_parameter)\n",
    "\n",
    "print(\"Loss = \",RegLoss(theta_hat, X_ext, y, regularization_parameter))\n",
    "PlotDecisionBoundary(X,y,theta_hat, degree=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>>>> Q8: Train the classifier with extended features, for regularization parameter=0 and for regularization parameter = 100. Plot the corresponding decision boundaries. Can you interpret the results?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regularization_parameter = 0.0\n",
    "# Start Edit...\n",
    "\n",
    "\n",
    "# ... End Edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regularization_parameter = 100\n",
    "# Start Edit...\n",
    "\n",
    "\n",
    "# ... End Edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Comparison with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train with original features\n",
    "regularization_parameter = 0.1\n",
    "\n",
    "initial_theta = np.zeros((X.shape[1],1))\n",
    "theta_hat = MinimizeRegLoss(initial_theta,X,y, regularization_parameter)\n",
    "\n",
    "classifier = LogisticRegression(fit_intercept=False, \n",
    "                                C = 1/regularization_parameter, \n",
    "                                solver='newton-cg')\n",
    "classifier.fit(X, np.ravel(y));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classifier.coef_)\n",
    "print(theta_hat)\n",
    "print(\"Loss (ours)    = \", RegLoss(theta_hat, X, y, regularization_parameter))\n",
    "print(\"Loss (sklearn) = \", RegLoss(classifier.coef_.T, X, y, regularization_parameter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train with extended features\n",
    "regularization_parameter = 0.1\n",
    "\n",
    "X_ext = MapFeatures(X[:,1],X[:,2],degree)\n",
    "initial_theta = np.zeros((X_ext.shape[1],1))\n",
    "theta_hat = MinimizeRegLoss(initial_theta,X_ext,y, regularization_parameter)\n",
    "\n",
    "classifier = LogisticRegression(fit_intercept=False, \n",
    "                                C = 1/regularization_parameter, \n",
    "                                solver='newton-cg')\n",
    "classifier.fit(X_ext, np.ravel(y));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classifier.coef_)\n",
    "print(theta_hat)\n",
    "print(\"\")\n",
    "print(\"Loss (ours)    = \", RegLoss(theta_hat, X_ext, y, regularization_parameter))\n",
    "print(\"Loss (sklearn) = \", RegLoss(classifier.coef_.T, X_ext, y, regularization_parameter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5. Multi-class Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Load Dataset 3. \n",
    "#### This is subset of the MNIST (Modified National Institute of Standards and Technology) dataset of handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"digits.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data3 = np.loadtxt('dataset3.csv', delimiter=',')\n",
    "print(data3[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate features (x) from labels (y)\n",
    "x=data3[:,:-1]\n",
    "y=data3[:,-1] # last column\n",
    "\n",
    "# Number of examples, Dimensions of feature space\n",
    "N, D = x.shape\n",
    "\n",
    "y = np.reshape(y,(N,1))   # re-shape y to column vector\n",
    "print(y[:10])\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"N. of examples in class {} = {}\".format(i,np.sum(y==i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add column of ones in front of x\n",
    "ones = np.ones((N,1))\n",
    "X = np.concatenate((ones,x),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train  Classifiers (one-vs-all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "all_theta = np.zeros((num_classes,D+1))\n",
    "\n",
    "regularization_parameter = 1.0\n",
    "\n",
    "for k in range(num_classes):\n",
    "    \n",
    "    initial_theta = np.zeros((X.shape[1],1))\n",
    "    \n",
    "    y_k = np.array([1 if label == k else 0 for label in y])\n",
    "    y_k = np.reshape(y_k, (X.shape[0], 1))\n",
    "    \n",
    "    all_theta[k,:] = MinimizeRegLoss(initial_theta,X,y_k, regularization_parameter)\n",
    "    print(\"Loss for class {} = {}\".format(k,RegLoss(all_theta[k], X, y_k, regularization_parameter)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Predicted Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = sigmoid(np.dot(X,all_theta.T))\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted = np.zeros((X.shape[0],1))\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    y_predicted[i] = np.argmax(h[i])\n",
    "\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_accuracy = np.mean(y_predicted==y)\n",
    "print('Total accuracy  = {:.3f}'.format(total_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>>>> Q9: What are the accuracies for each of the ten classes?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fraction = 0.8\n",
    "\n",
    "N = X.shape[0] # 5000\n",
    "train_size = int(N*train_fraction)\n",
    "test_size = N-train_size\n",
    "\n",
    "print(\"training set size = \",train_size)\n",
    "print(\"test set size = \",test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Random sampling from dataset\n",
    "X_resampled = []\n",
    "y_resampled = []\n",
    "for i in np.random.randint(N, size = N):\n",
    "    X_resampled.append(X[i])\n",
    "    y_resampled.append(y[i])\n",
    "\n",
    "X_resampled = np.array(X_resampled)\n",
    "y_resampled = np.array(y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>>>> Q10: Split 'X_resampled' and 'y_resampled' into training and test sets. You should define 'X_train', 'y_train', 'X_test', 'y_test'</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Train one-vs-all multi-class classifiers on training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>>>> Q11: Train one-vs-all classifiers on the training set, to obtain an 'all_theta' array of optimal parameters. 'all_theta' should be a matrix of size ($\\textrm{num}_{classes} \\times (D+1)$ )</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "all_theta = np.zeros((num_classes,D+1))\n",
    "\n",
    "regularization_parameter = 1.0\n",
    "\n",
    "# Start Edit...\n",
    "\n",
    "\n",
    "# ... End Edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Predicted Labels on Training and Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>>>> Q12: Predict labels on training and test sets. You should define 'y_pred_train' (of size (train_size x 1) ) and 'y_pred_test' (of size (test_size x 1) )</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>>>> Q13: Compute classification accuracy on training and test sets for training fraction=80% and for training fraction=50%</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_fraction = 0.8\n",
    "train_accuracy = 0.0 #?\n",
    "test_accuracy = 0.0 #?\n",
    "\n",
    "print('Training accuracy  = {:.3f}'.format(train_accuracy))\n",
    "print('Test accuracy  = {:.3f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_fraction = 0.5\n",
    "train_accuracy = 0.0 #?\n",
    "test_accuracy = 0.0 #?\n",
    "\n",
    "print('Training accuracy  = {:.3f}'.format(train_accuracy))\n",
    "print('Test accuracy  = {:.3f}'.format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
